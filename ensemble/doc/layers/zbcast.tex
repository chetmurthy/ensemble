\begin{Layer}{ZBCAST}

The ZBCAST layer implements a gossip-style probabilistically reliable
multicast protocol.  Unlike most other protocols in \ensemble, this
protocol admits a small, but non-zero probability of message loss: a
message might be garbage collected even though some operational member
in the group has not received it yet.  We found that doing so can
offer dramatic improvements in the performance and scalability of the
protocol.


\begin{Protocol}
This protocol is composed of two sub-protocols structured roughly as in the
Internet MUSE protocol.  The first protocol is an unreliable multicast
protocol which makes a best-effort attempt to efficiently deliver each
message to its destinations.  The second protocol is a 2-phase
anti-entropy protocol that operates in a series of unsynchronized
rounds.  During each round, the first phase detects message loss; the
second phase corrects such losses and runs only if needed.
\end{Protocol}

\begin{Parameters}
\item zbcast\_fanout : the fanout of gossip messages.  This determines
how many destinations a member gossips to during each round.

\item zbcast\_sweep: the interval of each round.

\item zbcast\_idle: how many rounds to wait after the last
retransmission request of a message before that message can be garbage
collected.

\item zbcast\_max\_polls: the maximum number of destinations a member
can poll for missing messages during one round.

\item zbcast\_max\_reqs: the maximum number of retransmission requests
a member can make during one round.

\item zbcast\_max\_entropy: the maximum amount of data a member can
retransmit during one round.

\item zbcast\_req\_limit: the threshold for message retransmission
request before that request is multicasted to the whole group.

\item zbcast\_reply\_limit: the threshold for message retransmission
before that retransmission is multicasted to the whole group.
\end{Parameters}

\begin{Properties}
\item
Under some conservative assumptions about the network properties,
message delivery can be proved to have a bimodal distribution under
this protocol: with a very small probability the message will be
delivered to a small number of destinations(including failed ones);
with very high probability the message will be delivered to almost all
destinations; and with vanishingly low probability the message will be
delivered to \emph{many} but not \emph{most} destinations.

\item
Using this protocol for multicast transmissions, virtual synchrony
cannot be guaranteed since it admits a non-zero probability of message
losses at some operational members.  Message losses (if any) are
reported to the application.  If the message loss is deemed to
compromise correct behavior, the application may decide to leave the
group and then rejoins them, triggering state transfer -- a separate
feature provided by \ensemble.

\item
This protocol needs multicast support from underlying layers.  If
IP-multicast is not available, GCAST protocol is needed to simulate
the effect of multicast by a series of unicasts.

\item
As its current implementation, this protocol requires \emph{groupd}
membership services.

\item
This protocol assumes that the application is able to control its
message transmission within a certain rate (rate-based flow control).
If the load injected into the network is heavier than what it can
sustain, the failure probability and latency guarantees of the
protocol may no longer hold.
\end{Properties}

\begin{Sources}
\sourcesfile{zbcast.ml}
\end{Sources}

\begin{GenEvent}
\genevent{\Dn{Cast}}
\genevent{\Dn{Send}}
\genevent{\Up{LostMessage}}
\end{GenEvent}

\begin{Testing}
\item
Extensive experiments have been conducted on a SP2 parallel machine
(used as a network of UNIX workstations) with group size ranging from
$8$ to $128$ nodes.  The protocol scales gracefully and maintains
stable throughput.
\item
The protocol has been tested on a multicast-capable LAN with 30
Solaris workstations.  One of the member is the sender and the rest
are receivers.  The sender is sending at a rate of 200 messages per
second.  Each message is $1000$ bytes.  Most of the receivers are able
to maintain a steady throughput of $200$ msgs/sec.  Message losses are
very rare.

We emphasize that in both tests we have reached the limit of the
largest group of machines which we have access to.  We believe that
our protocol can scale far more than what is indicated above.
\item
In the next step of our work, we will investigate its performance on WAN.
\end{Testing}

\emph{This layer and its documentation were written by Zhen Xiao.  It
is based on the \emph{PBCAST} protocol implemented by Mark Hayden.
This documentation is based the \emph{Bimodal Multicast} paper.}
\end{Layer}
